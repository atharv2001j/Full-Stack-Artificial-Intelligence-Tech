{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "from model.yolo_model import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    \"\"\"\n",
    "    Resize, reduce and expand image\n",
    "    \n",
    "    #argument:\n",
    "    img: origial image.\n",
    "    #returns\n",
    "    image: ndarray(64,64,3), processed Image.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    image=cv2.resize(img,(416,416),\n",
    "                    interpolation=cv2.INTER_CUBIC)\n",
    "    image=np.array(image, dtype='float32')\n",
    "    image/=255\n",
    "    image=np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes an original image img as input and performs several processing steps on it.\n",
    "\n",
    "Resize: The cv2.resize() function resizes the input image to a specified size (416, 416). \n",
    "It uses the cv2. INTER CUBIC interpolation method, which is a higher-quality interpolation\n",
    "method compared to others like cv2.INT ER LINEAR.\n",
    "\n",
    "Data type conversion: The resized image is then converted to a NumPy arr ay with dtype='float32'. \n",
    "This step ensures that the image data type is s uitable for further processing, typically in\n",
    "machine learning or compute r vision tasks.\n",
    "\n",
    "Normalization: The pixel values of the image are normalized by dividing each pixel value by 255. This step scales the pixel values to the range [0, 1], which is a common practice for neural network input data normali zation.\n",
    "\n",
    "Dimension expansion: Finally, np.expand_dims() is used to add an extra d imension to the processed image array. This is often necessary when deal ing with batch processing in deep learning frameworks, where the first d Imension represents the batch size. In this case, axis indicates that the new dimension is added as the first dimension.\n",
    "\n",
    "The function returns the processed image with the specified shape (1, 416, 416, 3), where 1 represents the batch size, 416 and 416 represent the image dimensions, and 3 represents the number of color channels (RGB).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(file):\n",
    "    \"\"\"Get classes name.\n",
    "    \n",
    "    #Arugment:\n",
    "    file: classes name for databases\n",
    "    \n",
    "    #return:\n",
    "    class_names: list, classes name\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(file) as f:\n",
    "        class_names= f.readlines()\n",
    "    class_names= [c.strip() for c in class_names]\n",
    "    \n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to read a file containing class names and return them as a list.\n",
    "\n",
    "Opening the File: The function takes a parameter file, which is the path to the file containing the class names. It then opens this file using a with statement, which ensures that the file is properly closed after its suite finishes, even if an exception is raised.\n",
    "\n",
    "Reading Class Names: Inside the with block, f.readlines() reads all the lines from the file and returns them as a list of strings. Each string r epresents a class name.\n",
    "\n",
    "Stripping Newlines: Since readlines() includes the newline character \\n at the end of each line, the list of class names may contain trailing whitespace. The list comprehension [c.strip() for c in class_names] is use d to remove leading and trailing whitespace (including newlines) from ea ch class name.\n",
    "\n",
    "Returning Class Names: The function then returns the list of class name s.\n",
    "\n",
    "Overall, this function is a simple utility for extracting class names from a file. It's commonly used in machine learning and computer vision tasks where class names are stored externally, such as in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function , draw(image , boxes , scores, classes , all_classes),\n",
    "is intended to draw bounding boxes around detected objects on an iimage \n",
    "along with their corresponding class labels and confidence scores'''\n",
    "\n",
    "\n",
    "def draw(image, boxes, scores, classes, all_classes):\n",
    "    '''\n",
    "    Draw the boxes on the image\n",
    "    \n",
    "    #Arguments:\n",
    "    image: original image\n",
    "    boxes: ndarray , boxes of object\n",
    "    classes: ndarray, classes of objects\n",
    "    scores: ndarray, scores of objects\n",
    "    all_classes: all classes name\n",
    "    '''\n",
    "    for box, score, cl in zip(boxes, scores, classes):\n",
    "        x,y,w,h=box\n",
    "        \n",
    "        top=max(0, np.floor(x+0.5) . astype(int))\n",
    "        left=max(0, np.floor(y+0.5) . astype(int))\n",
    "        right=min(image.shape[1], np.floor(x+w+0.5).astype(int))\n",
    "        bottom=min(image.shape[1], np.floor(y+h+0.5).astype(int))\n",
    "        \n",
    "        \n",
    "        cv2.rectangle(image, (top, left), (right, bottom), (255,0,0), 2)\n",
    "        cv2.putText(image,'{0} {1:.2f}'. format(all_classes[cl],score),\n",
    "                   (top, left-6),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                   0.6,(0,0,255),1,\n",
    "                   cv2.LINE_AA)\n",
    "        \n",
    "        print('class:{0}, score:{1:.2f}'.format(all_classes[cl], score))\n",
    "        print('box coordinate x,y,w,h:{0}'.format(box))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to draw bounding boxes around detected objects on an image along with their labels and confidence scores.\n",
    "Iterating Over Detected Objects:\n",
    "\n",
    "It iterates over each detected object using a for loop and zip(boxe s, scores, classes), where boxes, scores, and classes are arrays contain ing the bounding box coordinates, confidence scores, and class indices o f detected objects, respectively.\n",
    "\n",
    "Bounding Box Coordinates:\n",
    "\n",
    "For each detected object, it unpacks the bounding box coordinates (x, y, w, h) from box.\n",
    "\n",
    "It calculates the top-left (top, left) and bottom-right (right, bott om) coordinates of the bounding box. These coordinates are used to drawthe rectangle and bounding box.\n",
    "\n",
    "Drawing Bounding Boxes:\n",
    "\n",
    "Using cv2.rectangle(), it draws a rectangle around the detected obje ct on the image. The rectangle is drawn using coordinates (top, left) an d (right, botton) with a blue celer (255, 0, 0) and a thickness of 2 pix els.\n",
    "\n",
    "Annotating with Class Label and Score:\n",
    "\n",
    "It annotates the bounding box with the class label and confidence sc ore using cv2.putText(). The label and score are formatted with the clas nane obtained from all classes[cl] (where cl is the class index) and t he confidence score. This annotation is placed slightly above the top-le ft corner of the bounding box with a red color (0, 0, 255) and a font sc ale of 0.6.\n",
    "\n",
    "Printing Information:\n",
    "\n",
    "For each detected object, it prints the class name and confidence sc ore along with the bounding box coordinates.\n",
    "\n",
    "Blank Line:\n",
    "\n",
    "Finally, it prints a blank line to separate the output for different images if multiple images are being processed.\n",
    "\n",
    "This function is commonly used in object detection tasks to visualize the detected objects on an image for validation or debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(image, yolo, all_classes):\n",
    "    \"\"\"Use yolo v3 to detect images.\n",
    "    # Argument:\n",
    "        image: original image.\n",
    "        yolo: YOLO, yolo model.\n",
    "        all_classes: all classes name.\n",
    "    # Returns:\n",
    "        image: processed image.\n",
    "    \"\"\"\n",
    "    pimage = process_image(image)\n",
    "    #The image is first processed into a format that the YOLO model #expects, typically involving resizing,\n",
    "    #normalizing, and possibly #transforming the image to fit the input shape for the YOLO model.\n",
    "    start = time.time()\n",
    "    boxes, classes, scores = yolo.predict(pimage, image.shape)\n",
    "    end = time.time()\n",
    "\n",
    "    print('time: {0:.2f}s'.format(end - start))\n",
    "# Here, the function calls YOLO's predict method on the processed image\n",
    "#(pimage). This method returns three things:\n",
    "#boxes: coordinates of bounding boxes for detected objects.\n",
    "#classes: the class indices (e.g., 'e' for 'person', '1' for 'car', etc.).\n",
    "#scores: confidence scores indicating how certain the model is that an object \n",
    "#The time taken for the detection is measured using time.time() to calculate\n",
    "    if boxes is not None:\n",
    "        draw(image, boxes, scores, classes, all_classes)\n",
    "#If YOLO detects any objects (i.e., boxes is not None),\n",
    "#the draw function is called. This function will typically:\n",
    "#If YOLO detects any objects (1.e., boxes is not None),\n",
    "#the draw function is called. This function will typically:\n",
    "#Draw bounding boxes on the original image.\n",
    "#Label each box with the class name (using all_classes).\n",
    "#Optionally include the confidence score\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_video(video, yolo, all_classes):\n",
    "    \"\"\"Use yolo v3 to detect video.\n",
    "\n",
    "    # Argument:\n",
    "        video: video file.\n",
    "        yolo: YOLO, yolo model.\n",
    "        all_classes: all classes name.\n",
    "    \"\"\"\n",
    "    video_path = os.path.join(\"C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/videos\",  \"C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/videos/test\", video)\n",
    "    camera = cv2.VideoCapture(video_path)\n",
    "    cv2.namedWindow(\"C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/detection\", cv2.WINDOW_AUTOSIZE)\n",
    "    #fource= cv2.VideoWriter_fourcc(**mpeg') specifies the codec 28 to be used for saving the video (in this case, the 'moea codec).\n",
    "    #video_path constructs the path to the input video file by joining folder\n",
    "    #names and the video filename.\n",
    "    #camera cv2.VideoCapture(video_path) initializes the video capture object #using OpenCV (cv2). This allows reading frames from the video.\n",
    "    #cv2.namedWindow creates a window named \"detection\" for displaying the video\n",
    "    # Prepare for saving the detected video\n",
    "    sz = (int(camera.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mpeg')\n",
    "    #sz stores the dimensions of the video (width and height),\n",
    "    #which are retrieved using OpenCV's camera.get method.\n",
    "    \n",
    "    vout = cv2.VideoWriter()\n",
    "    vout.open(os.path.join(\"C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/videos\",  \"C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/videos/res\", video), fourcc, 20, sz, True)\n",
    "    \n",
    "    #vout = cv2.VideoWriter() creates a video writer object,\n",
    "    #and vout.open opens a new video file where the processed frames (with object\n",
    "    while True:\n",
    "        res, frame = camera.read()\n",
    "\n",
    "        if not res:\n",
    "            break\n",
    "\n",
    "        image = detect_image(frame, yolo, all_classes)\n",
    "        cv2.imshow(\"C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/detection\", image)\n",
    "    #The function detect_image(frame, yolo, all_classes) (the one we discussed eai\n",
    "    #The processed frame (with bounding boxes and Labels) is stored in image.\n",
    "    #cv2.imshow(\"detection\", image) displays the frame in the \"detection\" window\n",
    "        # Save the video frame by frame\n",
    "        vout.write(image)\n",
    "\n",
    "        if cv2.waitKey(110) & 0xff == 27:\n",
    "                break\n",
    "\n",
    "    vout.release()\n",
    "    camera.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo= YOLO(0.6, 0.5)\n",
    "file=\"C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/data/coco_classes.txt\"\n",
    "all_classes=get_classes(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting the Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.85s\n",
      "class:person, score:1.00\n",
      "box coordinate x,y,w,h:[187.71986008  84.54499674  91.60767555 304.36181641]\n",
      "class:horse, score:1.00\n",
      "box coordinate x,y,w,h:[396.45050049 137.28213882 215.7049942  208.54855251]\n",
      "class:dog, score:1.00\n",
      "box coordinate x,y,w,h:[ 61.28294468 263.38461542 145.58371544  88.16218674]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f='C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/images/test/person.jpg'\n",
    "#path\n",
    "image=cv2.imread('C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/images/test/person.jpg')\n",
    "image=detect_image(image, yolo , all_classes)\n",
    "cv2.imwrite('C:/Artificial Intelligence/2_Computer_Vision/3_Yolov_model/YOLOv3/images/res/'+f, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
